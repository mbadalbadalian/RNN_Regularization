{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Libraries and Additional Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import re, os, pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_PKL(curr_DICT,path):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(curr_DICT, file)\n",
    "    return curr_DICT\n",
    "\n",
    "def load_PKL(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        curr_DICT = pickle.load(file)\n",
    "    return curr_DICT\n",
    "\n",
    "def setup_device():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define Variable Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "torch.manual_seed(1234)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1234)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "params_unregularized = {\n",
    "    \"model_name\": \"lang_model_LSTM_unregularized\",\n",
    "    \"save_interval\": 10,\n",
    "    \"batch_size\": 20,\n",
    "    \"seq_length\": 20,\n",
    "    \"layers\": 2,\n",
    "    \"decay\": 2,\n",
    "    \"rnn_size\": 200,\n",
    "    \"dropout\": 0.0,\n",
    "    \"init_weight\": 0.1,\n",
    "    \"lr\": 1.0,\n",
    "    \"vocab_size\": 12000,\n",
    "    \"max_epoch\": 4,\n",
    "    \"max_max_epoch\": 13,\n",
    "    \"max_grad_norm\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_regularized = {\n",
    "    \"model_name\": \"lang_model_LSTM_regularized\",\n",
    "    \"save_interval\": 10,\n",
    "    \"batch_size\": 20,\n",
    "    \"seq_length\": 35,\n",
    "    \"layers\": 2,\n",
    "    \"decay\": 1.2,\n",
    "    \"rnn_size\": 650,\n",
    "    \"dropout\": 0.5,\n",
    "    \"init_weight\": 0.05,\n",
    "    \"lr\": 1.0,\n",
    "    \"vocab_size\": 12000,\n",
    "    \"max_epoch\": 6,\n",
    "    \"max_max_epoch\": 39,\n",
    "    \"max_grad_norm\": 5,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data,vocab_size):\n",
    "    vocab_map = {}\n",
    "    vocab_idx = 0\n",
    "    \n",
    "    word_freq = Counter(word for word in data)\n",
    "    \n",
    "    most_common_words = [word for word, _ in word_freq.most_common(vocab_size - 1)]  # Leave space for <unk>\n",
    "    \n",
    "    for word in most_common_words:\n",
    "        vocab_map[word] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "    \n",
    "    vocab_map['<unk>'] = vocab_idx\n",
    "    return vocab_map\n",
    "\n",
    "def load_data(data_type,params):    \n",
    "    data = load_dataset(\"tiny_shakespeare\", trust_remote_code=True)[data_type][\"text\"][0]\n",
    "    \n",
    "    data = data.replace('\\n',' <eos> ')\n",
    "    data = data.replace(\"'\",\" '\")\n",
    "    data = data.replace(\"--\",\" --\")\n",
    "    data = re.sub(r'[,!?;:]', '', data)\n",
    "    data = re.sub(r'[.,!?;:]+(?=\\s<eos>)', '', data)\n",
    "    data = data.lower()\n",
    "    data = data.split()\n",
    "    \n",
    "    vocab_map = build_vocab(data,params[\"vocab_size\"])\n",
    "            \n",
    "    x = torch.tensor([vocab_map.get(word, vocab_map['<unk>']) for word in data], dtype=torch.long)\n",
    "    return x\n",
    "\n",
    "def replicate(x_inp,batch_size):\n",
    "    s = x_inp.size(0)\n",
    "    x = torch.zeros((s//batch_size, batch_size), dtype=torch.long)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        start = i*s//batch_size  # Mimic Lua rounding\n",
    "        finish = start + x.size(0)\n",
    "        x[:, i] = x_inp[start:finish]\n",
    "    return x\n",
    "\n",
    "def traindataset(params):\n",
    "    x = load_data(\"train\",params)\n",
    "    x = replicate(x,params[\"batch_size\"])\n",
    "    return x\n",
    "\n",
    "def validdataset(params):\n",
    "    x = load_data(\"validation\",params)\n",
    "    x = replicate(x,params[\"batch_size\"])\n",
    "    return x\n",
    "\n",
    "def testdataset(params):\n",
    "    x = load_data(\"test\",params)\n",
    "    x = x.view(-1, 1).expand(-1,params[\"batch_size\"]).clone()\n",
    "    return x\n",
    "\n",
    "def getdatasets(params):\n",
    "    data_train = traindataset(params)\n",
    "    data_valid = validdataset(params)\n",
    "    data_test = testdataset(params)\n",
    "    return data_train, data_valid, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call dataset functions\n",
    "data_train_unregularized, data_valid_unregularized, data_test_unregularized = getdatasets(params_unregularized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_regularized, data_valid_regularized, data_test_regularized = getdatasets(params_regularized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LSTM Cell Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Cell implementation\n",
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.W = nn.Linear(input_size, 4*hidden_size)\n",
    "        self.U = nn.Linear(hidden_size, 4*hidden_size)\n",
    "    \n",
    "    def forward(self, x, prev_c, prev_h):\n",
    "        gates = self.W(x) + self.U(prev_h)\n",
    "        i, f, o, g = gates.chunk(4, dim=-1)\n",
    "        i, f, o = torch.sigmoid(i), torch.sigmoid(f), torch.sigmoid(o)\n",
    "        g = torch.tanh(g)\n",
    "        next_c = f * prev_c + i * g\n",
    "        next_h = o * torch.tanh(next_c)\n",
    "        return next_c, next_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(params[\"vocab_size\"], params[\"rnn_size\"])\n",
    "        self.lstm_cells = nn.ModuleList([\n",
    "            LSTMCell(params[\"rnn_size\"], params[\"rnn_size\"]) for _ in range(params[\"layers\"])\n",
    "        ])\n",
    "        self.fc = nn.Linear(params[\"rnn_size\"], params[\"vocab_size\"])\n",
    "        self.dropout = nn.Dropout(params[\"dropout\"])\n",
    "        self.init_weights(params)\n",
    "\n",
    "    def init_weights(self, params):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-params[\"init_weight\"], params[\"init_weight\"])\n",
    "\n",
    "    def forward(self, x, hidden_states):\n",
    "        x = self.embedding(x)\n",
    "        next_hidden_states = []\n",
    "        for i, cell in enumerate(self.lstm_cells):\n",
    "            prev_c, prev_h = hidden_states[i]\n",
    "            next_c, next_h = cell(x, prev_c, prev_h)\n",
    "            x = next_h  # Input for next layer\n",
    "            next_hidden_states.append((next_c, next_h))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return torch.log_softmax(x, dim=-1), next_hidden_states\n",
    "\n",
    "    def init_hidden(self, batch_size, params):\n",
    "        return [(torch.zeros(batch_size, params[\"rnn_size\"]).to(device),\n",
    "                 torch.zeros(batch_size, params[\"rnn_size\"]).to(device))\n",
    "                for _ in range(params[\"layers\"])]\n",
    "        \n",
    "    def save(self, params, epoch_num, metrics_DICT, best_model=False):\n",
    "        if best_model:\n",
    "            model_path = f\"models/{params['model_name']}_best.pth\"\n",
    "            metrics_path = f\"data/output_data/{params['model_name']}_best.pkl\"\n",
    "        else:\n",
    "            model_path = f\"models/{params['model_name']}_epoch_{epoch_num + 1}.pth\"\n",
    "            metrics_path = f\"data/output_data/{params['model_name']}_epoch_{epoch_num + 1}.pkl\"\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "        save_PKL(metrics_DICT, metrics_path)\n",
    "        return\n",
    "    \n",
    "def check_and_load_model(params,epoch_num,best_model=False,load_model=True):\n",
    "    if best_model:\n",
    "        model_path = f\"models/{params['model_name']}_best.pth\"\n",
    "        metrics_path = f\"data/output_data/{params['model_name']}_best.pkl\"\n",
    "    else:\n",
    "        model_path = f\"models/{params['model_name']}_epoch_{epoch_num + 1}.pth\"\n",
    "        metrics_path = f\"data/output_data/{params['model_name']}_epoch_{epoch_num + 1}.pkl\"\n",
    "    model = LSTMModel(params)\n",
    "    if os.path.exists(model_path) and load_model:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    if os.path.exists(model_path) and load_model:\n",
    "        metrics_DICT = load_PKL(metrics_path)\n",
    "    else:\n",
    "        metrics_DICT = {\"epoch_list\": [], \"train_ppl\": [], \"valid_ppl\": [], \"test_ppl\": 0, \"best_valid_ppl\": np.inf}\n",
    "    device = setup_device()\n",
    "    model.to(device) \n",
    "    return model,metrics_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "epoch_num_unregularized = 0\n",
    "criterion = nn.NLLLoss()\n",
    "model_unregularized,metrics_unregularized_DICT = check_and_load_model(params_unregularized,epoch_num_unregularized)\n",
    "optimizer_unregularized = optim.SGD(model_unregularized.parameters(), lr=params_unregularized[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "epoch_num_regularized = 0\n",
    "model_regularized,metrics_regularized_DICT = check_and_load_model(params_regularized,epoch_num_regularized)\n",
    "optimizer_regularized = optim.SGD(model_regularized.parameters(), lr=params_regularized[\"lr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp(data, model, params, hidden_states):\n",
    "    model.train()\n",
    "    x = data[:-1]\n",
    "    y = data[1:]\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    output, hidden_states = model(x, hidden_states)\n",
    "    loss = criterion(output.view(-1, params[\"vocab_size\"]), y.view(-1))\n",
    "    return loss, hidden_states\n",
    "\n",
    "def bp(loss, model, params, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), params[\"max_grad_norm\"])\n",
    "    optimizer.step()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "def run_epoch(data, model, params, optimizer):\n",
    "    hidden_states = model.init_hidden(params[\"batch_size\"],params)\n",
    "    total_loss = 0\n",
    "    num_batches = max(1, data.size(0) // params[\"seq_length\"] - 1)\n",
    "    for batch in range(num_batches):\n",
    "        x = data[batch * params[\"seq_length\"]:(batch + 1) * params[\"seq_length\"]]\n",
    "        y = data[batch * params[\"seq_length\"] + 1:(batch + 1) * params[\"seq_length\"] + 1]\n",
    "        loss, hidden_states = fp(x, model, params, hidden_states)\n",
    "        bp(loss, model, params, optimizer)\n",
    "        total_loss += loss.item()\n",
    "        hidden_states = [(c.detach(), h.detach()) for (c, h) in hidden_states]\n",
    "    return torch.exp(torch.tensor(total_loss / num_batches))\n",
    "\n",
    "def train(data_train, data_valid, model, metrics_DICT, params, optimizer):\n",
    "    if not metrics_DICT[\"train_ppl\"]:\n",
    "        epoch_list = []\n",
    "        train_ppl_list = []\n",
    "        valid_ppl_list = []\n",
    "    else:\n",
    "        epoch_list = metrics_DICT[\"epoch_list\"].copy()\n",
    "        train_ppl_list = metrics_DICT[\"train_ppl\"].copy()\n",
    "        valid_ppl_list = metrics_DICT[\"valid_ppl\"].copy()\n",
    "    curr_epoch = len(metrics_DICT[\"epoch_list\"])\n",
    "    best_val_ppl = metrics_DICT[\"best_valid_ppl\"]\n",
    "    max_num_epochs = params[\"max_max_epoch\"]\n",
    "    for epoch in range(curr_epoch,params[\"max_max_epoch\"]):\n",
    "        train_ppl = run_epoch(data_train, model, params, optimizer)\n",
    "        valid_ppl = evaluate(data_valid, model, metrics_DICT, params)[0]\n",
    "        print(f'Epoch {epoch + 1}/{max_num_epochs}, Train PPL: {train_ppl:.4f}, Val PPL: {valid_ppl:.4f}')\n",
    "        epoch_list.append(epoch + 1)\n",
    "        train_ppl_list.append(train_ppl)\n",
    "        valid_ppl_list.append(valid_ppl)\n",
    "        if epoch > params[\"max_epoch\"]:\n",
    "            params[\"lr\"] /= params[\"decay\"]\n",
    "        if (epoch + 1) % params[\"save_interval\"] == 0 or epoch == params[\"max_max_epoch\"] - 1:\n",
    "            metrics_DICT[\"epoch_list\"] = epoch_list.copy()\n",
    "            metrics_DICT[\"train_ppl\"] = train_ppl_list.copy()\n",
    "            metrics_DICT[\"valid_ppl\"] = valid_ppl_list.copy()\n",
    "            model.save(params, epoch, metrics_DICT)\n",
    "        if valid_ppl < best_val_ppl:\n",
    "            best_val_ppl = valid_ppl\n",
    "            metrics_DICT[\"epoch_list\"] = epoch_list.copy()\n",
    "            metrics_DICT[\"train_ppl\"] = train_ppl_list.copy()\n",
    "            metrics_DICT[\"valid_ppl\"] = valid_ppl_list.copy()\n",
    "            metrics_DICT[\"best_valid_ppl\"] = best_val_ppl\n",
    "            metrics_DICT[\"best_epoch\"] = epoch + 1\n",
    "            model.save(params, epoch, metrics_DICT, best_model=True)\n",
    "    return model, params, metrics_DICT\n",
    "\n",
    "def evaluate(data, model, metrics_DICT, params, best_model=False):\n",
    "    model.eval()\n",
    "    hidden_states = model.init_hidden(params[\"batch_size\"],params)\n",
    "    total_loss = 0\n",
    "    num_batches = max(1, data.size(0) // params[\"seq_length\"] - 1)\n",
    "    with torch.no_grad():\n",
    "        for batch in range(num_batches):\n",
    "            x = data[batch * params[\"seq_length\"]:(batch + 1) * params[\"seq_length\"]]\n",
    "            y = data[batch * params[\"seq_length\"] + 1:(batch + 1) * params[\"seq_length\"] + 1]\n",
    "            loss, hidden_states = fp(x, model, params, hidden_states)\n",
    "            total_loss += loss.item()\n",
    "            hidden_states = [(c.detach(), h.detach()) for (c, h) in hidden_states]\n",
    "    eval_ppl = torch.exp(torch.tensor(total_loss/num_batches))\n",
    "    metrics_DICT[\"test_ppl\"] = eval_ppl\n",
    "    \n",
    "    if best_model:\n",
    "        epoch = metrics_unregularized_DICT[\"best_epoch\"]\n",
    "    else:\n",
    "        epoch = len(metrics_DICT[\"epoch_list\"])\n",
    "    model.save(params, epoch, metrics_DICT, best_model)\n",
    "    return eval_ppl, metrics_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13, Train PPL: 490.3062, Val PPL: 327.2083\n",
      "Epoch 2/13, Train PPL: 442.6985, Val PPL: 332.6233\n",
      "Epoch 3/13, Train PPL: 391.6645, Val PPL: 364.2330\n",
      "Epoch 4/13, Train PPL: 342.1205, Val PPL: 405.4155\n",
      "Epoch 5/13, Train PPL: 307.6555, Val PPL: 441.9352\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "model_unregularized, metrics_unregularized_DICT = train(data_train_unregularized, data_valid_unregularized, model_unregularized, metrics_unregularized_DICT, params_unregularized, optimizer_unregularized)\n",
    "\n",
    "# Evaluation loop\n",
    "best_model=True\n",
    "model_unregularized, metrics_unregularized_DICT = check_and_load_model(params_unregularized,metrics_unregularized_DICT[\"best_epoch\"],best_model)\n",
    "test_eval_ppl_unregularized, metrics_unregularized_DICT = evaluate(data_test_unregularized, model_unregularized, metrics_unregularized_DICT, params_unregularized, best_model)\n",
    "print(f\"Test PPL: {test_eval_ppl_unregularized:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model_regularized, metrics_regularized_DICT = train(data_train_regularized, data_valid_regularized, model_regularized, metrics_regularized_DICT, params_regularized, optimizer_regularized)\n",
    "\n",
    "# Evaluation loop\n",
    "best_model=True\n",
    "model_regularized, metrics_regularized_DICT = check_and_load_model(params_regularized,metrics_regularized_DICT[\"best_epoch\"],best_model)\n",
    "test_eval_ppl_regularized, metrics_regularized_DICT = evaluate(data_test_regularized, model_regularized, metrics_regularized_DICT, params_regularized, best_model)\n",
    "print(f\"Test PPL: {test_eval_ppl_regularized:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the train and validation loss\n",
    "def plot_train_valid(params,metrics_DICT):\n",
    "    plt.figure();\n",
    "    plt.plot(metrics_DICT['epoch_list'],metrics_DICT['train_loss_list'], label=f'Train PPL', color='blue', linestyle='--', marker='o');\n",
    "    plt.plot(metrics_DICT['epoch_list'],metrics_DICT['val_loss_list'], label=f'Validation PPL', color='green', linestyle='-', marker='x');\n",
    "    plt.title(f'{params[\"model_name\"]} Training and Validation PPL');\n",
    "    plt.xlabel('Epochs');\n",
    "    plt.ylabel('Loss');\n",
    "    plt.legend();\n",
    "    plt.grid();\n",
    "    plt.xlim(0,max(metrics_DICT['epoch_list'])+1);\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_valid(params_unregularized,metrics_unregularized_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_valid(params_regularized,metrics_regularized_DICT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNN_Dropout_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
